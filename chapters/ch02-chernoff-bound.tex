\chapter{Chernoff Bound}

In this Chapter we see a way to bound tail distributions, that is, the probability that a random variable assumes values that are far from its expectation.

\section{First-Edge Algorithm, revised}
In the last chapter, we studied the First-Edge Algorithm for reconstructing the social graph given a set of traces. The way we proved the algorithm to be valid can be abstracted in the following way. For each trace $i$, and a given edge $\{u,v\} \in E(G)$, define
\begin{equation}
X_i = \begin{cases}
1 & \{u, v\}\ \text{are not the first two nodes in trace } i\\
0 & \text{otherwise}. 
\end{cases}
\end{equation}

We considered only a given edge and we computed the probability that the nodes involved in it were never at the beginning of a trace. The reason we wanted to do that was that if $\{u, v\}$ were the first two nodes in a trace then the edge was indeed in the graph.

What we were really asking for is the following:
\begin{equation}
	\pr{\prod_{i} X_i = 1}.
\end{equation}
 
 If all $X_i$s are one then no trace had $\{u,v\}$ as first nodes. Otherwise, the product is zero and the edge is in the graph. Essentially we studied the product of random variables.

When there are a bunch of independent variables and one wants to compute, say, when one of them is different from the others, then the analysis is usually pretty simple. In most cases, though, is not so easy. Usually what is studied is the random variable $X$ defined as
\begin{equation}
	X=\sum_i X_i,
\end{equation}
that is, the sum of the $X_i$s as opposite to the product. In most cases, people study this random variable, defining if from a set of variables, which may be independent, and  with the aim to compute statistics like the expectation, the second moment, or the probability that the $X$ deviate from its expectation by more than a certain amount. If $X$ is always (or with high probability) close to its expectation we say the variable is \emph{concentrated}. This specific random variable is ubiquitous in algorithms and random processes. We will start looking at this type of variable and its use in modeling social network processes.


\section{Chernoff bound}

We state (without proof) the Chernoff Bound, which is a powerful tool to bound a tail distribution with exponentially decreasing values.
\begin{thm}[Chernoff Bound]
	Let $X_1, X_2, \ldots, X_n$, $0 \leq X_i \leq 1,\forall i$, be a set of mutually independent random variables, and $X = \sum_{i=1}^nX_i$. Then
	\begin{equation}
	\pr{X \leq (1-\varepsilon)E[X]} \leq e^{-\frac{\varepsilon^2E[X]}{3}}
	\end{equation}
	
	and
	\begin{equation}
	\pr{X \geq (1+\varepsilon)E[X]} \leq e^{-\frac{\varepsilon^2E[X]}{3}}.
	\end{equation}
\end{thm}

Consider the following situation. We flip $n$ fair coins. Define
\[X_i = \begin{cases}1 \textit{ if the i-th flip results in heads}\\0 \textit{ otherwise}\end{cases}\]

then 
\[X = \sum_{i=1}^n = \textit{ \# of heads}.\] 

What is the probability of the event $\{X = k\}$? For $X$ to be $k$ there must be exactly $k$ coins which resulted in heads. We can choose among $\binom{n}{k}$ compatible events (set of variables), each having the same probability $2^{-n}$ to happen. It follows that
\begin{equation*}
	\pr{X = k} = \binom{n}{k}2^{-n}.
\end{equation*} 

What if the coin is not fair? We have the binomial distribution.

\begin{equation}
	\pr{X = k} = \binom{n}{k} p^k(1-p)^{n-k}.
\end{equation}

We would like to know what is the probability that $X$ assumes a value far from the expected one. We can use the Chernoff Bound.

\begin{equation}
	\pr{X \leq (1-\varepsilon)np} \leq e^{-\frac{\varepsilon^2pn}{3}},
\end{equation}

The quantity is going down exponentially with $n$. Why is this useful? $\epsilon$ can go down to $\frac{1}{\sqrt(n)}$ and we would still get something that is going down to $0$ very quickly. Suppose we choose $p = \frac{1}{2}$ and
\begin{equation}
	\varepsilon = \sqrt{\frac{\log n}{n}}
\end{equation}

Then we have
\[e^{-\frac{\frac{\log n}{n}\frac{1}{2}n}{3}} = e^{-\frac{1}{6}\log n} = \frac{1}{\sqrt[6]{n}}.\]

That is, the probability of error can be made as tiny as we want by increasing $n$.

The Chernoff bound is an extremely important tool that is used, as an example, in all applications that deal with large datasets. This is because it states that it is not necessary to look at all the values to get a sense about what the mean is. 

Consider the following scenarion. There are a bunch of people that has to vote of one candidate or the other. A first attempt to predict the winner could be to ask to every person for his/her preference. Then we one would have all the necessary information, but it cannot be done in practice. A more realistic way of doing it is, because of the existence of the Chernoff bound, to pick random people. If there is some gap between the two candidates, maybe $51$ vs. $49$ or more, then after maybe few cases (maybe $2000$ people in this example) one would be able to tell the candidate that is more likely to win. The error may be proportional to $\sqrt{n}$ but, as long as the gap is linear in $n$, the prediction can be considered valid.


\section{Chain letters}

Chain letters are (were) online petitions that spread virally through emails. They work in the following way. The petition's creator writes an email explaining a certain problem he or she wants to address. Then he/she signs the petition, sends it to a set of recipients and ask them to sign and forward the email. Some recipients may ignore the email, others may sign and forward it instead. One can model the spread using a tree $T$, where the root is the petition's creator and every node has a child for each person he forwarded the email to. Of course, email are privates. For an external observer to know the petition, at least one person must post the email on a public accessible place on the Web (e.g. public mailing lists). When a node $v$ (person) \emph{exposes} itself (i.e. publish the email), all its ancestors in the tree are revealed too (the path from the root to $v$).

We would like to know some information about the spread process, such as the reach of the petition. The full tree $T$ is in general unknown, and what can be observed of it depends on how many and which nodes get exposed. We can assume that there exists a small probability $\delta > 0$ such that every node $v \in T$ gets exposed independently with that probability. Define $T_\delta$ to be the visible part of $T$ reconstructed though the exposed nodes. Let $V$ be the set of nodes in $T_\delta$. Define $E \subseteq V$ to be the set of nodes that exposed themselves by disclosing their email to the public. Finally, let $L \subseteq E$ be the set of leaves in $T_\delta$.

As we said, we would like to know $|V| = n$, but we only have $E$. Can we estimate $n$? If we assume that each node in $E$ exposed itself independently from the others, we can make a reasonable guess about $n$ by looking at $T_\delta$ \cite{chainletters}. In fact
\begin{equation}
 n\delta = |E|
\end{equation}
so 
\begin{equation}
n= \frac{|E|}{\delta}.
\end{equation}

At this point, we need to find $\delta'$ such that $\delta' \approx \delta$. One can wrongly think that
\begin{equation}
	\delta' = \frac{|E|}{|T_\delta|}.
\end{equation}

The problem here are the leaves of $T_\delta$. If they hadn't exposed themselves, we wouldn't know about them (or other nodes). From the point of view of $T_\delta$, they are \emph{not} exposed independently with probability $\delta'$; rather, they are exposed with probability $1$. To compute $\delta'$ then, we exclude $T_\delta$'s leaves. What follows is an algorithm that outputs $\delta'$.

\begin{algorithm}
\textbf{Input:} $E, L, V$.\\
\If{$|V| = 0$}{\Return $0$}
\If{$|V| = 1$}{\Return $1$}
\Return $\frac{|E| - |L|}{|V|-|L|}$

\caption{Estimation of $\delta$}
\end{algorithm}

Finally, if $|V| \geq 2$,

\begin{equation}
	 n \approx \frac{|E|}{\delta'}.
\end{equation}

\begin{lem}
	If $|V| \geq 2$ then
	\begin{equation}
	\pr{|\delta' - \delta| \geq \epsilon\delta} \leq 2e^{\frac{1}{3}\epsilon^2\delta|V-L|}.
	\end{equation}
\end{lem}
\begin{proof}
	For each node $i$, let $X_i$ be a random variable such that
	\begin{equation}
		X_i = \begin{cases}
		1 \textit{ if node i exposes itself}\\
		0 \textit{ otherwise}.
		\end{cases}
	\end{equation}
	
	Consider the three sets $A$, $B$ and $C$ built in the following way. We can scan the original tree in a bottom up fashion. At a certain time, node $i$ is considered. If $i \not\in A$, check if the node was exposed. If $X_i = 1$, add $i$ to $B$ and every its ancestor to $A$; otherwise, add $i$ to $C$. 
	Observe that the tripartition process does not disclose the value of $X_i$ for all $i \in A$, and that $A = V - L$ and thus the set $E - L$ coincides with the set of exposed nodes in $A$. Then, we can apply the Chernoff bound, having $X = |E - L|$.
	
	\begin{align}
		\pr{X \geq (1 + \epsilon)\delta |A|} &= \pr{|E - L| \geq |A|\delta + \epsilon\delta|A|}\\
		&=\pr{\frac{|E - L|}{|A|} \geq \delta + \epsilon\delta}\\
		&=\pr{|\delta'-\delta| \geq \epsilon\delta} \tag{We want the difference small enough}\\
		&= \pr{|X - |A|\delta| \geq \epsilon\delta|A|}\\
		&\leq 2e^{-\frac{1}{3}\epsilon^2E[X]}\\
		& = 2e^{-\frac{1}{3}\epsilon^2\delta|V-L|}.
		\end{align}	
\end{proof}

